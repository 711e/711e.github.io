---
layout: post
title:  "Interview"
crawlertitle: "Interview"
summary: "Interview"
date:   2019-04-24 10:37:00 +0700
categories: posts
tags: 'Interview'
author: 711E
---

### 一、机器学习
#### 1.线性回归

变量之间的对应函数关系
##### (1)函数模型
`h(x)=w0+w1x1+w2x2+...+wnxn`
`h(x)=WX`
##### (2)损失函数
最小二乘法：我们有很多的给定点，这时候我们需要找出一条线去拟合它，那么我先假设这个线的方程，然后把数据点代入假设的方程得到观测值，求使得实际值与观测值相减的平方和最小的参数。对变量求偏导联立便可求。

#### 2.逻辑回归

sigmoid

#### 3.朴素贝叶斯

假设有一个数据集，由两类组成（简化问题），对于每个样本的分类，我们都已经知晓。现在出现一个新的点new_point (x,y)，其分类未知。我们可以用p1(x,y)表示数据点(x,y)属于红色一类的概率，同时也可以用p2(x,y)表示数据点(x,y)属于蓝色一类的概率。那要把new_point归在红、蓝哪一类呢？

我们提出这样的规则：
 * 如果p1(x,y) > p2(x,y)，则(x,y)为红色一类。
 * 如果p1(x,y) <p2(x,y),  则(x,y)为蓝色一类。

#### 4.决策树

用决策树分类：从根节点开始，对实例的某一特征进行测试，根据测试结果将实例分配到其子节点，此时每个子节点对应着该特征的一个取值，如此递归的对实例进行测试并分配，直到到达叶节点，最后将实例分到叶节点的类中。
* 决策树学习的目标：根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。
* 决策树学习的本质：从训练集中归纳出一组分类规则，或者说是由训练数据集估计条件概率模型。
* 决策树学习的损失函数：正则化的极大似然函数
* 决策树学习的测试：最小化损失函数
* 决策树学习的目标：在损失函数的意义下，选择最优决策树的问题

#### 5.GBDT

GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。

#### 6.随机森林

每棵树的按照如下规则生成：

1. 如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本（这种采样方式称为bootstrap sample方法），作为该树的训练集；从这里我们可以知道：每棵树的训练集都是不同的，而且里面包含重复的训练样本（理解这点很重要）。

**为什么要随机抽样训练集？**

如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，这样的话完全没有bagging的必要；

**为什么要有放回地抽样？**

如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是"有偏的"，都是绝对"片面的"（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是"求同"，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是"盲人摸象"。

2. 如果每个样本的特征维度为M，指定一个常数m<<M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；

3. 每棵树都尽最大程度的生长，并且没有剪枝过程。
　　一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。

**随机森林分类效果（错误率）与两个因素有关：**

* 森林中任意两棵树的相关性：相关性越大，错误率越大；
* 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。
减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。

#### 7.Adaboosting

AdaBoost是典型的Boosting算法，属于Boosting家族的一员。在说AdaBoost之前，先说说Boosting提升算法。Boosting算法是将“弱学习算法“提升为“强学习算法”的过程，主要思想是“三个臭皮匠顶个诸葛亮”。一般来说，找到弱学习算法要相对容易一些，然后通过反复学习得到一系列弱分类器，组合这些弱分类器得到一个强分类器。Boosting算法要涉及到两个部分，加法模型和前向分步算法。加法模型就是说强分类器由一系列弱分类器线性相加而成。一般组合形式如下：
`FM(x;P)=∑m=1nβmh(x;am)`其中，h(x;am)就是一个个的弱分类器，am是弱分类器学习到的最优参数，βm就是弱学习在强分类器中所占比重，P是所有am和βm的组合。这些弱分类器线性相加组成强分类器。
前向分步就是说在训练过程中，下一轮迭代产生的分类器是在上一轮的基础上训练得来的。也就是可以写成这样的形式：`Fm(x)=Fm−1(x)+βmhm(x;am)`由于采用的损失函数不同，Boosting算法也因此有了不同的类型，AdaBoost就是损失函数为指数损失的Boosting算法。

**基于Boosting的理解，对于AdaBoost，我们要搞清楚两点：**
* 每一次迭代的弱学习h(x;am)有何不一样，如何学习？
* 弱分类器权值βm如何确定？
对于第一个问题，AdaBoost改变了训练数据的权值，也就是样本的概率分布，其思想是将关注点放在被错误分类的样本上，减小上一轮被正确分类的样本权值，提高那些被错误分类的样本权值。然后，再根据所采用的一些基本机器学习算法进行学习，比如逻辑回归。

对于第二个问题，AdaBoost采用加权多数表决的方法，加大分类误差率小的弱分类器的权重，减小分类误差率大的弱分类器的权重。这个很好理解，正确率高分得好的弱分类器在强分类器中当然应该有较大的发言权。
迭代t次：
1. 初始化D1=(w11,…，w1i,…，w1N)，w1i=1/N,i=1,2,…，N,给定一个二分类的训练数据集：T=(x1,y1),(x2,y2),……,(xN,yN),标记yi∈[−1,1],得到弱分类器Gt(x)
2. 分类误差et=P(Gt(x)!=yi)
3. Gt(x)的权值αt=1/2·log(1-et)/et,et≤1/2时，αt>0,并且随着et的减小而增大，也就是说分类误差越小分类器的权值越大，这里还可以看出可以看出权值分布Dt通过影响et来影响了αt，这是Dt的第一个影响。
4. 更新权值分布Dt+1(x)
得到强分类器：F(x)=sign(i~t∑(αtGt(x)))

#### 8.SVM
